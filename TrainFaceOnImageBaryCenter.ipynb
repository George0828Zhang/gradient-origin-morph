{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify from https://github.com/drorsimon/image_barycenters\n",
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCelebDataset(Dataset):\n",
    "    def __init__(self, root_name: str = \"img_align_celeba\", normalize = True, image_type=\"jpg\"):\n",
    "        # --------------------------------------------\n",
    "        # Initialize paths, transforms, and so on\n",
    "        # --------------------------------------------\n",
    "        # if root_name != \"src\" and root_name != \"adv_imgs\":\n",
    "        #     raise NotImplementError\n",
    "\n",
    "        self.img_paths = []\n",
    "        for path in glob.glob(\"./\"+root_name+\"/*.\"+image_type):\n",
    "\n",
    "            img_PIL = Image.open(path)\n",
    "            if(len(np.array(img_PIL).shape) == 3 and np.array(img_PIL).shape[2] == 3):\n",
    "                self.img_paths.append(path)\n",
    "\n",
    "        self.mean_rgb = None\n",
    "        self.std_rgb = None\n",
    "        \n",
    "        \n",
    "        if normalize:\n",
    "            self.transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])])\n",
    "        else:\n",
    "            self.transforms = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # --------------------------------------------\n",
    "        # 1. Read from file (using numpy.fromfile, PIL.Image.open)\n",
    "        # 2. Preprocess the data (torchvision.Transform).\n",
    "        # 3. Return the data (e.g. image and label)\n",
    "        # --------------------------------------------\n",
    "        img_path = self.img_paths[index]\n",
    "        img_PIL = Image.open(img_path)\n",
    "        # original image shape is : [3, 102, 136]\n",
    "        # I resize it to : [3, 64, 64] to have the same setting of https://github.com/drorsimon/image_barycenters/blob/master/generate_h5.py\n",
    "        img_PIL = img_PIL.resize((64, 64), Image.ANTIALIAS)\n",
    "        img_tensor = self.transforms(img_PIL)\n",
    "        \n",
    "       \n",
    "        return img_tensor\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # --------------------------------------------\n",
    "        # Indicate the total size of the dataset\n",
    "        # --------------------------------------------\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = myCelebDataset()\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                                  shuffle=True, num_workers=72, worker_init_fn=np.random.seed(1234))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, num_filters):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Hidden layers\n",
    "        self.hidden_layer = torch.nn.Sequential()\n",
    "        for i in range(len(num_filters)):\n",
    "            # Deconvolutional layer\n",
    "            if i == 0:\n",
    "                deconv = torch.nn.ConvTranspose2d(latent_dim, num_filters[i], kernel_size=4, stride=1, padding=0)\n",
    "            else:\n",
    "                deconv = torch.nn.ConvTranspose2d(num_filters[i-1], num_filters[i], kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "            deconv_name = 'deconv' + str(i + 1)\n",
    "            self.hidden_layer.add_module(deconv_name, deconv)\n",
    "\n",
    "            torch.nn.init.normal_(deconv.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(deconv.bias, 0.0)\n",
    "\n",
    "            # Batch normalization\n",
    "            bn_name = 'bn' + str(i + 1)\n",
    "            self.hidden_layer.add_module(bn_name, torch.nn.BatchNorm2d(num_filters[i]))\n",
    "\n",
    "            # Activation\n",
    "            act_name = 'act' + str(i + 1)\n",
    "            self.hidden_layer.add_module(act_name, torch.nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = torch.nn.Sequential()\n",
    "        \n",
    "        # Deconvolutional layer\n",
    "        out = torch.nn.ConvTranspose2d(num_filters[i], 3, kernel_size=4, stride=2, padding=1)\n",
    "        self.output_layer.add_module('out', out)\n",
    "        torch.nn.init.normal_(out.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(out.bias, 0.0)\n",
    "        \n",
    "        # Activation\n",
    "        self.output_layer.add_module('act', torch.nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer(x)\n",
    "        out = self.output_layer(x)\n",
    "        return out\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Hidden layers\n",
    "        self.hidden_layer = torch.nn.Sequential()\n",
    "        for i in range(len(num_filters)):\n",
    "            # Convolutional layer\n",
    "            if i == 0:\n",
    "                conv = torch.nn.Conv2d(3, num_filters[i], kernel_size=4, stride=2, padding=1)\n",
    "            else:\n",
    "                conv = torch.nn.Conv2d(num_filters[i-1], num_filters[i], kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "            conv_name = 'conv' + str(i + 1)\n",
    "            self.hidden_layer.add_module(conv_name, conv)\n",
    "\n",
    "            # Initializer\n",
    "            torch.nn.init.normal_(conv.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(conv.bias, 0.0)\n",
    "\n",
    "            # Batch normalization\n",
    "            if i > 0:\n",
    "                bn_name = 'bn' + str(i + 1)\n",
    "                self.hidden_layer.add_module(bn_name, torch.nn.BatchNorm2d(num_filters[i]))\n",
    "\n",
    "            # Activation\n",
    "            act_name = 'act' + str(i + 1)\n",
    "            self.hidden_layer.add_module(act_name, torch.nn.LeakyReLU(0.2))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = torch.nn.Sequential()\n",
    "        # Convolutional layer\n",
    "        out = torch.nn.Conv2d(num_filters[i], 1, kernel_size=4, stride=1, padding=0)\n",
    "        self.output_layer.add_module('out', out)\n",
    "        # Initializer\n",
    "        torch.nn.init.normal_(out.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(out.bias, 0.0)\n",
    "        # Activation\n",
    "        self.output_layer.add_module('act', torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer(x)\n",
    "        out = self.output_layer(x)\n",
    "        return out\n",
    "\n",
    "# Encoder model\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_filters, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Hidden layers\n",
    "        self.hidden_layer = torch.nn.Sequential()\n",
    "        for i in range(len(num_filters)):\n",
    "            # Convolutional layer\n",
    "            if i == 0:\n",
    "                conv = torch.nn.Conv2d(3, num_filters[i], kernel_size=4, stride=2, padding=1)\n",
    "            else:\n",
    "                conv = torch.nn.Conv2d(num_filters[i-1], num_filters[i], kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "            conv_name = 'conv' + str(i + 1)\n",
    "            self.hidden_layer.add_module(conv_name, conv)\n",
    "\n",
    "            # Initializer\n",
    "            torch.nn.init.normal_(conv.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(conv.bias, 0.0)\n",
    "\n",
    "            # Batch normalization\n",
    "            if i > 0:\n",
    "                bn_name = 'bn' + str(i + 1)\n",
    "                self.hidden_layer.add_module(bn_name, torch.nn.BatchNorm2d(num_filters[i]))\n",
    "\n",
    "            # Activation\n",
    "            act_name = 'act' + str(i + 1)\n",
    "            self.hidden_layer.add_module(act_name, torch.nn.LeakyReLU(0.2))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = torch.nn.Sequential()\n",
    "        # Convolutional layer\n",
    "        out = torch.nn.Conv2d(num_filters[i], latent_dim, kernel_size=4, stride=1, padding=0)\n",
    "        self.output_layer.add_module('out', out)\n",
    "        # Initializer\n",
    "        torch.nn.init.normal_(out.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(out.bias, 0.0)\n",
    "        # Activation\n",
    "        self.output_layer.add_module('bn_out', torch.nn.BatchNorm2d(latent_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer(x)\n",
    "        out = self.output_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "%matplotlib inline\n",
    "def show(img, fig_size=(12,8)):\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.figure(figsize=fig_size, dpi=100) \n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(latent_dim=100, num_filters=[1024, 512, 256, 128]).to(device)\n",
    "D = Discriminator(num_filters=[128, 256, 512, 1024])\n",
    "E = Encoder(num_filters=[128, 256, 512, 1024], latent_dim=100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# I have no idea why using AlexNet for latent space = =\n",
    "if torch.cuda.is_available():\n",
    "    alexnet = models.alexnet(pretrained=True).to(device)\n",
    "else:\n",
    "    alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = G.to(device)\n",
    "D = D.to(device)\n",
    "E = E.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "# Train GAN\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Schedulers\n",
    "G_scheduler = optim.lr_scheduler.MultiStepLR(G_optimizer, milestones=[25,50,75])\n",
    "D_scheduler = optim.lr_scheduler.MultiStepLR(D_optimizer, milestones=[25,50,75])\n",
    "\n",
    "# loss arrays\n",
    "D_avg_losses = []\n",
    "G_avg_losses = []\n",
    "\n",
    "save_dir = \"BaryCenter\"\n",
    "\n",
    "num_test_samples = 16\n",
    "latent_dim = 100\n",
    "fixed_noise = torch.randn(num_test_samples, latent_dim, 1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        D_epoch_losses = []\n",
    "        G_epoch_losses = []\n",
    "        trange = tqdm(trainloader)\n",
    "        for i, images in enumerate(trange):\n",
    "#             if i > 10:\n",
    "#                 break\n",
    "            mini_batch = images.size()[0]\n",
    "            x = images.to(device)\n",
    "\n",
    "            y_real = torch.ones(mini_batch, device=device)\n",
    "            y_fake = torch.zeros(mini_batch, device=device)\n",
    "\n",
    "            # Train discriminator\n",
    "            D_real_decision = D(x).squeeze()\n",
    "            D_real_loss = criterion(D_real_decision, y_real)\n",
    "\n",
    "            z = torch.randn(mini_batch, latent_dim, 1, 1).to(device)\n",
    "            generated_images = G(z)\n",
    "\n",
    "            D_fake_decision = D(generated_images).squeeze()\n",
    "            D_fake_loss = criterion(D_fake_decision, y_fake)\n",
    "\n",
    "            # Backprop\n",
    "            D_loss = D_real_loss + D_fake_loss\n",
    "            D.zero_grad()\n",
    "            if i%2 == 0:  # Update discriminator only once every 2 batches\n",
    "                D_loss.backward()\n",
    "                D_optimizer.step()\n",
    "\n",
    "            # Train generator\n",
    "            z = torch.randn(mini_batch, latent_dim, 1, 1).to(device)\n",
    "            generated_images = G(z)\n",
    "\n",
    "            D_fake_decision = D(generated_images).squeeze()\n",
    "            G_loss = criterion(D_fake_decision, y_real)\n",
    "\n",
    "            # Backprop Generator\n",
    "            D.zero_grad()\n",
    "            G.zero_grad()\n",
    "            G_loss.backward()\n",
    "            G_optimizer.step()\n",
    "\n",
    "            # loss values\n",
    "            D_epoch_losses.append(D_loss.data.item())\n",
    "            G_epoch_losses.append(G_loss.data.item())\n",
    "\n",
    "            trange.set_description('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
    "                % (epoch+1, num_epochs, i+1, len(trainloader), D_loss.data.item(), G_loss.data.item()))\n",
    "\n",
    "        D_avg_loss = torch.mean(torch.FloatTensor(D_epoch_losses)).item()\n",
    "        G_avg_loss = torch.mean(torch.FloatTensor(G_epoch_losses)).item()\n",
    "        D_avg_losses.append(D_avg_loss)\n",
    "        G_avg_losses.append(G_avg_loss)\n",
    "        \n",
    "        G.eval()\n",
    "        generated_images = G(fixed_noise).detach()\n",
    "        G.train()\n",
    "        \n",
    "        show(make_grid(generated_images.cpu(), nrow=4, normalize=False, scale_each=False, range=(-1,1)))\n",
    "\n",
    "        # Save models\n",
    "        torch.save(G.state_dict(), os.path.join(save_dir,'generator'))\n",
    "        torch.save(D.state_dict(), os.path.join(save_dir,'discriminator'))\n",
    "\n",
    "        # Decrease learning-rate\n",
    "        G_scheduler.step()\n",
    "        D_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Encoder with noise\n",
    "for param in G.parameters():\n",
    "    param.requires_grad = False\n",
    "        \n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "E_optimizer = optim.Adam(E.parameters(), lr=2e-4, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "\n",
    "E_avg_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    E_losses = []\n",
    "\n",
    "    # minibatch training\n",
    "    trange = tqdm(trainloader)\n",
    "    for i, images in enumerate(trange):\n",
    "#         if i > 10:\n",
    "#             break\n",
    "        # generate_noise\n",
    "        z = torch.randn(images.shape[0],latent_dim,1,1).to(device)\n",
    "        x = G(z)\n",
    "\n",
    "        # Train Encoder\n",
    "        out_latent = E(x)\n",
    "        E_loss = criterion(z, out_latent)\n",
    "\n",
    "        # Back propagation\n",
    "        E.zero_grad()\n",
    "        E_loss.backward()\n",
    "        E_optimizer.step()\n",
    "\n",
    "        # loss values\n",
    "        E_losses.append(E_loss.data.item())\n",
    "\n",
    "        trange.set_description('Epoch [%d/%d], Step [%d/%d], E_loss: %.4f'\n",
    "            % (epoch+1, num_epochs, i+1, len(trainloader), E_loss.data.item()))\n",
    "\n",
    "    E_avg_loss = torch.mean(torch.FloatTensor(E_losses)).item()\n",
    "\n",
    "    # avg loss values for plot\n",
    "    E_avg_losses.append(E_avg_loss)\n",
    "\n",
    "    # Save models\n",
    "    torch.save(E.state_dict(), os.path.join(save_dir,'encoder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune_encoder_with_samples\n",
    "\n",
    "# load alexnet:\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "G.eval()\n",
    "for param in G.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Load encoder    \n",
    "E.train()\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Optimizers\n",
    "E_optimizer = optim.Adam(E.parameters(), lr=1e-4, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "\n",
    "E_avg_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet_norm(x): \n",
    "    assert x.max() <= 1 or x.min() >= 0, f\"Alexnet received input outside of range [0,1]: {x.min(),x.max()}\"\n",
    "    out = x - torch.tensor([0.485, 0.456, 0.406]).reshape(1,3,1,1).type_as(x)\n",
    "    out = out / torch.tensor([0.229, 0.224, 0.225]).reshape(1,3,1,1).type_as(x)\n",
    "    return out\n",
    "def denorm(x):\n",
    "    return x/2+0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "interpolate = lambda x: F.interpolate(x, scale_factor=4, mode='bilinear')\n",
    "get_features = lambda x: alexnet.features(alexnet_norm(interpolate(denorm(x))))\n",
    "for epoch in range(num_epochs):\n",
    "    E_losses = []\n",
    "\n",
    "    # minibatch training\n",
    "    trange = tqdm(trainloader)\n",
    "    for i, images in enumerate(trange):\n",
    "#         if i > 10:\n",
    "#             break\n",
    "\n",
    "        # generate_noise\n",
    "        mini_batch = images.size()[0]\n",
    "        x = images.to(device)\n",
    "\n",
    "        # Train Encoder\n",
    "        out_images = G(E(x))\n",
    "        E_loss = criterion(x, out_images) + 0.002*criterion(get_features(x), get_features(out_images))\n",
    "\n",
    "        # Backprop\n",
    "        E.zero_grad()\n",
    "        E_loss.backward()\n",
    "        E_optimizer.step()\n",
    "\n",
    "        # loss values\n",
    "        E_losses.append(E_loss.data.item())\n",
    "\n",
    "        trange.set_description('Epoch [%d/%d], Step [%d/%d], E_loss: %.4f'\n",
    "            % (epoch+1, num_epochs, i+1, len(trainloader), E_loss.data.item()))\n",
    "\n",
    "    E_avg_loss = torch.mean(torch.FloatTensor(E_losses)).item()\n",
    "\n",
    "    # avg loss values for plot\n",
    "    E_avg_losses.append(E_avg_loss)\n",
    "\n",
    "    # Save models\n",
    "    torch.save(E.state_dict(), os.path.join(save_dir,'encoder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
